{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "from RL.helpers import energy_price_env\n",
    "from Hack import load\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, new_env=None, num_episodes=100, index=None):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    if new_env is None:\n",
    "        env = model.get_env()\n",
    "    else:\n",
    "        env = new_env\n",
    "    env.reset()\n",
    "    all_episode_rewards = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        episode_rewards = []\n",
    "\n",
    "        if i == 0:\n",
    "            current_prices = []\n",
    "            mean_prices = []\n",
    "            current_energies = []\n",
    "            all_earnings = [0]\n",
    "            current_times = []\n",
    "\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            current_price, mean_price, current_energy, current_time = (\n",
    "                obs[0, 0],\n",
    "                obs[0, 1],\n",
    "                obs[0, 2],\n",
    "                obs[0, 3],\n",
    "            )\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if i == 0:\n",
    "                if len(current_energies) > 0:\n",
    "                    all_earnings.append(\n",
    "                        -current_price * (current_energy - current_energies[-1])\n",
    "                    )\n",
    "\n",
    "                current_prices.append(current_price)\n",
    "                mean_prices.append(mean_price)\n",
    "                current_energies.append(current_energy)\n",
    "                current_times.append(current_time)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, sharex=True)\n",
    "    if index is None:\n",
    "        index = np.arange(0, len(current_times))[:-1]\n",
    "    else:\n",
    "        index = index[np.asarray(current_times, dtype=int)][:-1]\n",
    "    cum_rewards = np.cumsum(episode_rewards)\n",
    "    bank_total = np.cumsum(all_earnings)\n",
    "    axs[0].plot(index, cum_rewards[:-1], color=\"red\", label=\"Cumalative rewards\")\n",
    "    axs[0].plot(index, bank_total[:-1], color=\"blue\", label=\"Bank total\")\n",
    "    axs[0].legend()\n",
    "    axs[1].plot(index, current_prices[:-1], color=\"blue\", label=\"Current prices\")\n",
    "    axs[1].plot(index, mean_prices[:-1], color=\"red\", label=\"Mean prices\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(index, episode_rewards[:-1], color=\"black\", label=\"Reward\")\n",
    "    axs[2].legend()\n",
    "\n",
    "    axs[3].plot(index, current_energies[:-1], color=\"blue\", label=\"Current energies\")\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    std_episode_reward = np.std(all_episode_rewards)\n",
    "    print(\n",
    "        \"Mean reward:\",\n",
    "        mean_episode_reward,\n",
    "        \"+/-\",\n",
    "        std_episode_reward,\n",
    "        \"\\t Num episodes:\",\n",
    "        num_episodes,\n",
    "    )\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgmf4\\anaconda3\\envs\\ml\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "BEFORE\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgmf4\\Documents\\AI_Hack\\Hackathon\\debug_rolling_mean.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000002?line=12'>13</a>\u001b[0m model1\u001b[39m=\u001b[39m PPO(MlpPolicy, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# default\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000002?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBEFORE\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000002?line=15'>16</a>\u001b[0m mean_reward_before_train \u001b[39m=\u001b[39m evaluate(model1, num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, index \u001b[39m=\u001b[39;49m epex\u001b[39m.\u001b[39;49mindex)\n",
      "\u001b[1;32mc:\\Users\\mgmf4\\Documents\\AI_Hack\\Hackathon\\debug_rolling_mean.ipynb Cell 2'\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, new_env, num_episodes, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000001?line=51'>52</a>\u001b[0m             current_times\u001b[39m.\u001b[39mappend(current_time)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000001?line=53'>54</a>\u001b[0m     all_episode_rewards\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(episode_rewards))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000001?line=55'>56</a>\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m, sharex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000001?line=56'>57</a>\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgmf4/Documents/AI_Hack/Hackathon/debug_rolling_mean.ipynb#ch0000001?line=57'>58</a>\u001b[0m     index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(current_times))[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# define the data\n",
    "epex = load.epex().load()\n",
    "price_array = epex['apx_da_hourly'].values\n",
    "\n",
    "# define environment\n",
    "#! window_size is sort of a free parameter\n",
    "max_time = 30769 \n",
    "env = energy_price_env(price_array, max_time=max_time, window_size=24*2)\n",
    "\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# define the model \n",
    "model1= PPO(MlpPolicy, env, verbose=1) # default\n",
    "\n",
    "print(\"BEFORE\")\n",
    "mean_reward_before_train = evaluate(model1, num_episodes=1, index = epex.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = 48*7\n",
    "new_env =  DummyVecEnv([lambda: energy_price_env(price_array, start_time=max_time, max_time = periods)])\n",
    "print(\"AFTER\")\n",
    "mean_reward_after_train = evaluate(model1, new_env=new_env, num_episodes=1, index=epex.index)\n",
    "# load model using loaded_model = PPO.load(\"path_to_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "040d274fdfca6ecc88f65f18dafc70b49547e52dd567f9545727ec9f8e0b0ee0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

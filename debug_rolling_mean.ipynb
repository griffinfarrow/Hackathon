{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "from RL.helpers import energy_price_env\n",
    "from Hack import load\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, new_env=None, num_episodes=100, index=None):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    if new_env is None:\n",
    "        env = model.get_env()\n",
    "    else:\n",
    "        env = new_env\n",
    "    env.reset()\n",
    "    all_episode_rewards = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        episode_rewards = []\n",
    "\n",
    "        if i == 0:\n",
    "            current_prices = []\n",
    "            mean_prices = []\n",
    "            current_energies = []\n",
    "            all_earnings = [0]\n",
    "            current_times = []\n",
    "\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            current_price, mean_price, current_energy, current_time = (\n",
    "                obs[0, 0],\n",
    "                obs[0, 1],\n",
    "                obs[0, 2],\n",
    "                obs[0, 3],\n",
    "            )\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if i == 0:\n",
    "                if len(current_energies) > 0:\n",
    "                    all_earnings.append(\n",
    "                        -current_price * (current_energy - current_energies[-1])\n",
    "                    )\n",
    "\n",
    "                current_prices.append(current_price)\n",
    "                mean_prices.append(mean_price)\n",
    "                current_energies.append(current_energy)\n",
    "                current_times.append(current_time)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, sharex=True)\n",
    "    if index is None:\n",
    "        index = np.arange(0, len(current_times))[:-1]\n",
    "    else:\n",
    "        index = index[np.asarray(current_times, dtype=int)][:-1]\n",
    "    cum_rewards = np.cumsum(episode_rewards)\n",
    "    bank_total = np.cumsum(all_earnings)\n",
    "    axs[0].plot(index, cum_rewards[:-1], color=\"red\", label=\"Cumalative rewards\")\n",
    "    axs[0].plot(index, bank_total[:-1], color=\"blue\", label=\"Bank total\")\n",
    "    axs[0].legend()\n",
    "    axs[1].plot(index, current_prices[:-1], color=\"blue\", label=\"Current prices\")\n",
    "    axs[1].plot(index, mean_prices[:-1], color=\"red\", label=\"Mean prices\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(index, episode_rewards[:-1], color=\"black\", label=\"Reward\")\n",
    "    axs[2].legend()\n",
    "\n",
    "    axs[3].plot(index, current_energies[:-1], color=\"blue\", label=\"Current energies\")\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    std_episode_reward = np.std(all_episode_rewards)\n",
    "    print(\n",
    "        \"Mean reward:\",\n",
    "        mean_episode_reward,\n",
    "        \"+/-\",\n",
    "        std_episode_reward,\n",
    "        \"\\t Num episodes:\",\n",
    "        num_episodes,\n",
    "    )\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgmf4\\anaconda3\\envs\\ml\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "BEFORE\n",
      "Mean reward: -482.9073 +/- 0.0 \t Num episodes: 1\n"
     ]
    }
   ],
   "source": [
    "# define the data\n",
    "epex = load.epex().load()\n",
    "price_array = epex['apx_da_hourly'].values\n",
    "\n",
    "# define environment\n",
    "#! window_size is sort of a free parameter\n",
    "max_time = 30769 \n",
    "env = energy_price_env(price_array, max_time=max_time, window_size=24*2)\n",
    "\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# define the model \n",
    "model1= PPO(MlpPolicy, env, verbose=1) # default\n",
    "\n",
    "print(\"BEFORE\")\n",
    "mean_reward_before_train = evaluate(model1, num_episodes=1, index = epex.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1732 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1162        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010128422 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.00384     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00539    |\n",
      "|    value_loss           | 26.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 982          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030682576 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.000986     |\n",
      "|    value_loss           | 23.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 937          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069247726 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 18.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 910         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010677723 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.02        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 20.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1e75bef0760>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgmf4\\anaconda3\\envs\\ml\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER\n",
      "Mean reward: -108.68419 +/- 0.0 \t Num episodes: 1\n"
     ]
    }
   ],
   "source": [
    "periods = 48*7\n",
    "new_env =  DummyVecEnv([lambda: energy_price_env(price_array, start_time=max_time, max_time = periods)])\n",
    "print(\"AFTER\")\n",
    "mean_reward_after_train = evaluate(model1, new_env=new_env, num_episodes=1, index=epex.index)\n",
    "# load model using loaded_model = PPO.load(\"path_to_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "040d274fdfca6ecc88f65f18dafc70b49547e52dd567f9545727ec9f8e0b0ee0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

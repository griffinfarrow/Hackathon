{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Hack import load\n",
    "\n",
    "epex = load.epex().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ronan\\Anaconda3\\envs\\ml\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "stop_loss = 100\n",
    "\n",
    "price_array = epex['apx_da_hourly'].values\n",
    "\n",
    "def get_price(idx):\n",
    "    return price_array[idx]\n",
    "    \n",
    "def get_mean_price(idx):\n",
    "    window_size = 1000\n",
    "    if idx == 0:\n",
    "        return price_array[idx]\n",
    "    elif idx < window_size:\n",
    "        return np.mean(price_array[:idx])\n",
    "    else:\n",
    "        return np.mean(price_array[idx-window_size:idx])\n",
    "\n",
    "class energy_price_env(gym.Env):\n",
    "    \n",
    "    # ### Action Space\n",
    "    # The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "    # | Num | Action                 |\n",
    "    # |-----|------------------------|\n",
    "    # | 0   | Push cart to the left  |\n",
    "    # | 1   | Push cart to the right |\n",
    "    \n",
    "    def __init__(self, start_energy=1, start_time=0, max_time = 7*24*2):\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        # current_price, mean_price, current_energy\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([-np.inf, -np.inf, 0]),\n",
    "                                                high=np.array([np.inf, np.inf, 1]),\n",
    "                                                dtype=np.float32)\n",
    "        # our state is the charge\n",
    "        self.start_energy = start_energy\n",
    "        self.state = np.array([get_price(start_time), get_mean_price(start_time), start_energy])\n",
    "        self.time = start_time\n",
    "        self.earnings = 0\n",
    "        self.power = 1 #MW\n",
    "        self.capacity = 1 #MWh\n",
    "        self.efficiency = 0.85 \n",
    "        self.max_time = max_time\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price, mean_price, current_energy = self.state\n",
    "        mapped_action = env2human(action)\n",
    "        if mapped_action == -1:\n",
    "            # discharge === selling for 30 mins\n",
    "            new_energy = current_energy - (self.power * 0.5)\n",
    "            \n",
    "        elif mapped_action == 0:\n",
    "            # hold === do nothing\n",
    "            new_energy = current_energy\n",
    "        elif mapped_action == 1:\n",
    "            # charge === buy energy for 30 mins\n",
    "            new_energy = current_energy + (self.power * 0.5 * self.efficiency)\n",
    "        \n",
    "        \n",
    "        # make sure energy cannot be greater than capacity\n",
    "        new_energy = max(0, new_energy)\n",
    "        new_energy = min(self.capacity, new_energy)\n",
    "        # now work out the delta energy\n",
    "        delta_energy = (new_energy - current_energy)\n",
    "        \n",
    "        \n",
    "        revenue = - delta_energy * current_price\n",
    "        self.earnings += revenue\n",
    "        expected_profit = - delta_energy * mean_price\n",
    "        \n",
    "        opportunity_cost = revenue - expected_profit\n",
    "        \n",
    "        reward = opportunity_cost # profit * multiplier * price_diff_from_expected\n",
    "\n",
    "        # print(\"Delta energy: \", delta_energy)\n",
    "        # print(\"Price diff from expected: \", price_diff_from_expected)\n",
    "        # print(\"Revenue: \", revenue)\n",
    "        # print(\"Expected Profit: \", expected_profit)\n",
    "        # print(\"Reward \", reward)\n",
    "        \n",
    "        # increase the time\n",
    "        self.time += 1\n",
    "        \n",
    "        self.state = (get_price(self.time), get_mean_price(self.time), new_energy)\n",
    "        \n",
    "        info = {}\n",
    "        if self.time >= self.max_time:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return np.array(self.state), reward, done, info\n",
    "        \n",
    "    def reset(self):\n",
    "        # this resets the environment so it can try again\n",
    "        # print('Environment reset')\n",
    "        self.state = np.array([get_price(0), get_mean_price(0), self.start_energy])\n",
    "        self.time = 0\n",
    "        self.earnings = 0\n",
    "        return self.state\n",
    "\n",
    "def humans2env(action):\n",
    "    return int(action+1)\n",
    "\n",
    "def env2human(action):\n",
    "    return int(action-1)\n",
    "\n",
    "check_env(energy_price_env(1), warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# env = DummyVecEnv([lambda: energy_price_env(1)])\n",
    "env = energy_price_env()\n",
    "model = PPO(MlpPolicy, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reset() got an unexpected keyword argument 'start_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMean reward:\u001b[39m\u001b[39m\"\u001b[39m, mean_episode_reward, \u001b[39m\"\u001b[39m\u001b[39mNum episodes:\u001b[39m\u001b[39m\"\u001b[39m, num_episodes)\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m mean_episode_reward\n\u001b[1;32m---> 70\u001b[0m mean_reward_before_train \u001b[39m=\u001b[39m evaluate(model, num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, start_time, max_time, num_episodes)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# This function will only work for a single Environment\u001b[39;00m\n\u001b[0;32m     12\u001b[0m env \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_env()\n\u001b[1;32m---> 13\u001b[0m env\u001b[39m.\u001b[39;49mreset(start_time \u001b[39m=\u001b[39;49m start_time)\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m max_time \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     env\u001b[39m.\u001b[39mmax_time \u001b[39m=\u001b[39m max_time\n",
      "\u001b[1;31mTypeError\u001b[0m: reset() got an unexpected keyword argument 'start_time'"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate(model, start_time=0, max_time=None, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    env.reset(start_time = start_time)\n",
    "    if max_time is not None:\n",
    "        env.max_time = max_time\n",
    "    all_episode_rewards = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        episode_rewards = []\n",
    "        if i == 0:\n",
    "            current_prices = []\n",
    "            mean_prices = []\n",
    "            current_energies = []\n",
    "            all_earnings = [0]\n",
    "        \n",
    "        done = False\n",
    "        obs = env.reset(start_time = start_time)\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            current_price, mean_price, current_energy = obs[0,0], obs[0,1], obs[0,2]\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if i == 0:\n",
    "                if len(current_energies)>0:\n",
    "                    all_earnings.append(-current_price*(current_energy-current_energies[-1]))\n",
    "                \n",
    "                current_prices.append(current_price)\n",
    "                mean_prices.append(mean_price)\n",
    "                current_energies.append(current_energy)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    fig, axs = plt.subplots(4,1,sharex = True)\n",
    "    index = np.arange(0, len(current_energies))\n",
    "    cum_rewards = np.cumsum(episode_rewards)\n",
    "    bank_total = np.cumsum(all_earnings)\n",
    "    axs[0].plot(index, cum_rewards, color = 'red', label='Cumalative rewards')\n",
    "    axs[0].plot(index, bank_total, color = 'blue', label='Bank total')\n",
    "    axs[0].legend()\n",
    "    axs[1].plot(index, current_prices, color = 'blue', label='Current prices')\n",
    "    axs[1].plot(index, mean_prices, color = 'red', label='Mean prices')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    axs[2].plot(index, episode_rewards, color = 'black', label = 'Reward')\n",
    "    axs[2].legend()\n",
    "    \n",
    "    axs[3].plot(index, current_energies, color = 'blue', label='Current energies')\n",
    "\n",
    "    \n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "\n",
    "    return mean_episode_reward\n",
    "\n",
    "mean_reward_before_train = evaluate(model, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 336      |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 1541     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21cbdbbb370>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 57.600433 Num episodes: 1\n"
     ]
    }
   ],
   "source": [
    "# Trained Agent, after training\n",
    "mean_reward_after_train = evaluate(model, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'energy_price_env' object has no attribute 'set_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ronan\\Projects\\Hackathon\\Learn_RL.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ronan/Projects/Hackathon/Learn_RL.ipynb#ch0000013?line=0'>1</a>\u001b[0m periods_in_week \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m\u001b[39m*\u001b[39m\u001b[39m24\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ronan/Projects/Hackathon/Learn_RL.ipynb#ch0000013?line=1'>2</a>\u001b[0m \u001b[39m# set the start of the test data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ronan/Projects/Hackathon/Learn_RL.ipynb#ch0000013?line=2'>3</a>\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mset_time(periods_in_week)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ronan/Projects/Hackathon/Learn_RL.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39m# how far into the future\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ronan/Projects/Hackathon/Learn_RL.ipynb#ch0000013?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(periods_in_week):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'energy_price_env' object has no attribute 'set_time'"
     ]
    }
   ],
   "source": [
    "periods_in_week = 7*24*2\n",
    "# set the start of the test data\n",
    "obs = env.set_time(periods_in_week)\n",
    "# how far into the future\n",
    "for i in range(periods_in_week):\n",
    "    action, _states = model.predict(obs)\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1149646f41e957ad8a138086d8dcbe68460d591d12e33bd334a567b3e069840"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

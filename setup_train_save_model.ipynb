{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "import numpy as np\n",
    "from RL import helpers\n",
    "from Hack import load\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data\n",
    "epex = load.epex().load()\n",
    "price_array = epex['apx_da_hourly'].values\n",
    "\n",
    "# define environment\n",
    "#! window_size is sort of a free parameter\n",
    "max_time = 31*48#30769 \n",
    "env = helpers.energy_price_env(price_array, max_time=max_time, window_size=24*2)\n",
    "\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model \n",
    "model1= PPO(MlpPolicy, env, verbose=1) # default\n",
    "\n",
    "\n",
    "#model1.save(\"../model_dir/trained_model1\")\n",
    "# Trained Agent, after training\n",
    "periods = 48*7\n",
    "new_env =  DummyVecEnv([lambda: helpers.energy_price_env(price_array, start_time=max_time, max_time = periods)])\n",
    "print(\"AFTER\")\n",
    "mean_reward_after_train = helpers.evaluate(model1, new_env=new_env, num_episodes=10, index=epex.index)\n",
    "# load model using loaded_model = PPO.load(\"path_to_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEFORE\")\n",
    "mean_reward_before_train = helpers.evaluate(model1, num_episodes=10, index = epex.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model1.learn(total_timesteps=10000)\n",
    "# save the trained model\n",
    "model1.save(../model_dir/final_trained_model.zip)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "040d274fdfca6ecc88f65f18dafc70b49547e52dd567f9545727ec9f8e0b0ee0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
